{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install munch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlrQ0xoxkLV0",
        "outputId": "a73a01cf-eca3-49b7-f165-0ac7a2880b23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting munch\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: munch\n",
            "Successfully installed munch-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SYj8cBuN3Ndy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906c77e6-35dc-4ac9-f382-9f40b35c91d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DefaultMunch(None, {'data_path': '/content/Dataset', 'year': '2022', 'img_size': 128, 'batch_size_GAN': 64, 'epochs_GAN': 10000, 'lr_GAN': 0.0001, 'latent_dim': 32, 'n_critic': 5, 'clip_value': 0.01, 'load_G_name': 'my_G3900.pth', 'epochs_lstm': 30000, 'batch_size_lstm': 4, 'lr_lstm': 0.01, 'J': 8, 'hidden_size': 64, 'layer': 2})\n",
            "Found 6 images in /content/Dataset\n",
            "[Epoch 0/10000] [Iter 0] [D loss: 0.005471652373671532] [G loss: 0.008700779639184475]\n",
            "[Epoch 100/10000] [Iter 100] [D loss: -71.4735107421875] [G loss: -168.00198364257812]\n",
            "[Epoch 200/10000] [Iter 200] [D loss: -108.00687408447266] [G loss: 95.29097747802734]\n",
            "[Epoch 300/10000] [Iter 300] [D loss: -73.1285400390625] [G loss: 671.4341430664062]\n",
            "[Epoch 400/10000] [Iter 400] [D loss: -189.16217041015625] [G loss: 111.56846618652344]\n",
            "[Epoch 500/10000] [Iter 500] [D loss: -105.32147216796875] [G loss: -361.9626159667969]\n",
            "[Epoch 600/10000] [Iter 600] [D loss: -98.5716552734375] [G loss: -423.93853759765625]\n",
            "[Epoch 700/10000] [Iter 700] [D loss: -395.23876953125] [G loss: -376.69921875]\n",
            "[Epoch 800/10000] [Iter 800] [D loss: -290.651611328125] [G loss: 319.239501953125]\n",
            "[Epoch 900/10000] [Iter 900] [D loss: 41.646728515625] [G loss: 142.89285278320312]\n",
            "[Epoch 1000/10000] [Iter 1000] [D loss: -317.94140625] [G loss: -601.2445678710938]\n",
            "[Epoch 1100/10000] [Iter 1100] [D loss: -651.3912353515625] [G loss: -10.646566390991211]\n",
            "[Epoch 1200/10000] [Iter 1200] [D loss: -58.729736328125] [G loss: -155.4507598876953]\n",
            "[Epoch 1300/10000] [Iter 1300] [D loss: -409.3268127441406] [G loss: 502.2964782714844]\n",
            "[Epoch 1400/10000] [Iter 1400] [D loss: -540.0034790039062] [G loss: -565.634765625]\n",
            "[Epoch 1500/10000] [Iter 1500] [D loss: -600.4449462890625] [G loss: 164.1588134765625]\n",
            "[Epoch 1600/10000] [Iter 1600] [D loss: -367.922119140625] [G loss: -253.79937744140625]\n",
            "[Epoch 1700/10000] [Iter 1700] [D loss: -360.69964599609375] [G loss: -199.77499389648438]\n",
            "[Epoch 1800/10000] [Iter 1800] [D loss: -570.7322387695312] [G loss: 5.8335981369018555]\n",
            "[Epoch 1900/10000] [Iter 1900] [D loss: -270.37359619140625] [G loss: -314.2176208496094]\n",
            "[Epoch 2000/10000] [Iter 2000] [D loss: -306.59368896484375] [G loss: 95.20010375976562]\n",
            "[Epoch 2100/10000] [Iter 2100] [D loss: -272.0472106933594] [G loss: -320.62799072265625]\n",
            "[Epoch 2200/10000] [Iter 2200] [D loss: -534.4970703125] [G loss: -181.11099243164062]\n",
            "[Epoch 2300/10000] [Iter 2300] [D loss: -339.7308654785156] [G loss: 291.1724853515625]\n",
            "[Epoch 2400/10000] [Iter 2400] [D loss: -136.1068115234375] [G loss: 129.3267822265625]\n",
            "[Epoch 2500/10000] [Iter 2500] [D loss: -164.032470703125] [G loss: -274.86212158203125]\n",
            "[Epoch 2600/10000] [Iter 2600] [D loss: -437.2978820800781] [G loss: -204.69549560546875]\n",
            "[Epoch 2700/10000] [Iter 2700] [D loss: -97.7216796875] [G loss: 98.83724975585938]\n",
            "[Epoch 2800/10000] [Iter 2800] [D loss: -566.4077758789062] [G loss: 936.1588745117188]\n",
            "[Epoch 2900/10000] [Iter 2900] [D loss: -600.6678466796875] [G loss: 170.57464599609375]\n",
            "[Epoch 3000/10000] [Iter 3000] [D loss: -581.8766479492188] [G loss: -249.00962829589844]\n",
            "[Epoch 3100/10000] [Iter 3100] [D loss: -406.1826477050781] [G loss: 110.94517517089844]\n",
            "[Epoch 3200/10000] [Iter 3200] [D loss: -242.910400390625] [G loss: -442.6503601074219]\n",
            "[Epoch 3300/10000] [Iter 3300] [D loss: -680.1505737304688] [G loss: 66.70045471191406]\n",
            "[Epoch 3400/10000] [Iter 3400] [D loss: -410.8271484375] [G loss: -768.4881591796875]\n",
            "[Epoch 3500/10000] [Iter 3500] [D loss: -428.44140625] [G loss: -975.4989624023438]\n",
            "[Epoch 3600/10000] [Iter 3600] [D loss: -801.71923828125] [G loss: 76.43745422363281]\n",
            "[Epoch 3700/10000] [Iter 3700] [D loss: -313.80902099609375] [G loss: 105.48443603515625]\n",
            "[Epoch 3800/10000] [Iter 3800] [D loss: -240.30197143554688] [G loss: -148.18467712402344]\n",
            "[Epoch 3900/10000] [Iter 3900] [D loss: -401.78826904296875] [G loss: -212.8990478515625]\n",
            "[Epoch 4000/10000] [Iter 4000] [D loss: -288.1203308105469] [G loss: -409.0157165527344]\n",
            "[Epoch 4100/10000] [Iter 4100] [D loss: -287.05023193359375] [G loss: 373.05657958984375]\n",
            "[Epoch 4200/10000] [Iter 4200] [D loss: -695.8387451171875] [G loss: 559.8499145507812]\n",
            "[Epoch 4300/10000] [Iter 4300] [D loss: -137.037109375] [G loss: -557.3642578125]\n",
            "[Epoch 4400/10000] [Iter 4400] [D loss: -365.702880859375] [G loss: -1067.7958984375]\n",
            "[Epoch 4500/10000] [Iter 4500] [D loss: -175.8590087890625] [G loss: -116.65010833740234]\n",
            "[Epoch 4600/10000] [Iter 4600] [D loss: -150.4068603515625] [G loss: -1111.40869140625]\n",
            "[Epoch 4700/10000] [Iter 4700] [D loss: -189.62615966796875] [G loss: -167.5491943359375]\n",
            "[Epoch 4800/10000] [Iter 4800] [D loss: -433.58001708984375] [G loss: 37.50876998901367]\n",
            "[Epoch 4900/10000] [Iter 4900] [D loss: -227.0885009765625] [G loss: -948.2757568359375]\n",
            "[Epoch 5000/10000] [Iter 5000] [D loss: -305.396728515625] [G loss: -606.1096801757812]\n",
            "[Epoch 5100/10000] [Iter 5100] [D loss: -349.239501953125] [G loss: -1144.28369140625]\n",
            "[Epoch 5200/10000] [Iter 5200] [D loss: -39.12469482421875] [G loss: -0.215087890625]\n",
            "[Epoch 5300/10000] [Iter 5300] [D loss: -215.9198455810547] [G loss: 458.5653991699219]\n",
            "[Epoch 5400/10000] [Iter 5400] [D loss: -334.7466125488281] [G loss: -623.3797607421875]\n",
            "[Epoch 5500/10000] [Iter 5500] [D loss: -451.89813232421875] [G loss: -887.0729370117188]\n",
            "[Epoch 5600/10000] [Iter 5600] [D loss: -543.6063232421875] [G loss: -388.5990905761719]\n",
            "[Epoch 5700/10000] [Iter 5700] [D loss: -178.5882568359375] [G loss: -106.7511215209961]\n",
            "[Epoch 5800/10000] [Iter 5800] [D loss: -171.24920654296875] [G loss: -191.09967041015625]\n",
            "[Epoch 5900/10000] [Iter 5900] [D loss: -12.196044921875] [G loss: -487.88720703125]\n",
            "[Epoch 6000/10000] [Iter 6000] [D loss: -342.8651123046875] [G loss: -1325.283935546875]\n",
            "[Epoch 6100/10000] [Iter 6100] [D loss: -209.9945068359375] [G loss: -760.5643310546875]\n",
            "[Epoch 6200/10000] [Iter 6200] [D loss: -193.60476684570312] [G loss: -117.21420288085938]\n",
            "[Epoch 6300/10000] [Iter 6300] [D loss: -146.44537353515625] [G loss: -759.1063842773438]\n",
            "[Epoch 6400/10000] [Iter 6400] [D loss: -293.827392578125] [G loss: 157.77853393554688]\n",
            "[Epoch 6500/10000] [Iter 6500] [D loss: -358.4417724609375] [G loss: -491.7998046875]\n",
            "[Epoch 6600/10000] [Iter 6600] [D loss: -228.8768310546875] [G loss: -814.6102905273438]\n",
            "[Epoch 6700/10000] [Iter 6700] [D loss: -73.4541015625] [G loss: -1212.056884765625]\n",
            "[Epoch 6800/10000] [Iter 6800] [D loss: -599.655029296875] [G loss: -459.59466552734375]\n",
            "[Epoch 6900/10000] [Iter 6900] [D loss: -287.572509765625] [G loss: -250.22894287109375]\n",
            "[Epoch 7000/10000] [Iter 7000] [D loss: -213.7899169921875] [G loss: 780.9434814453125]\n",
            "[Epoch 7100/10000] [Iter 7100] [D loss: -65.46923828125] [G loss: -735.6903076171875]\n",
            "[Epoch 7200/10000] [Iter 7200] [D loss: -821.367431640625] [G loss: -756.657470703125]\n",
            "[Epoch 7300/10000] [Iter 7300] [D loss: -260.36651611328125] [G loss: -797.0678100585938]\n",
            "[Epoch 7400/10000] [Iter 7400] [D loss: -85.4176025390625] [G loss: 388.645263671875]\n",
            "[Epoch 7500/10000] [Iter 7500] [D loss: -310.9419860839844] [G loss: 333.7571716308594]\n",
            "[Epoch 7600/10000] [Iter 7600] [D loss: -344.8416748046875] [G loss: -1070.411865234375]\n",
            "[Epoch 7700/10000] [Iter 7700] [D loss: -206.5609130859375] [G loss: 431.9964294433594]\n",
            "[Epoch 7800/10000] [Iter 7800] [D loss: -194.1370849609375] [G loss: -697.1432495117188]\n",
            "[Epoch 7900/10000] [Iter 7900] [D loss: -375.4564208984375] [G loss: -1372.1767578125]\n",
            "[Epoch 8000/10000] [Iter 8000] [D loss: -185.1486358642578] [G loss: 792.8818359375]\n",
            "[Epoch 8100/10000] [Iter 8100] [D loss: -318.15716552734375] [G loss: 435.538818359375]\n",
            "[Epoch 8200/10000] [Iter 8200] [D loss: -280.9384765625] [G loss: 164.50991821289062]\n",
            "[Epoch 8300/10000] [Iter 8300] [D loss: -115.313232421875] [G loss: -1273.78076171875]\n",
            "[Epoch 8400/10000] [Iter 8400] [D loss: 82.41651153564453] [G loss: -467.4273681640625]\n",
            "[Epoch 8500/10000] [Iter 8500] [D loss: -219.05535888671875] [G loss: -649.1265258789062]\n",
            "[Epoch 8600/10000] [Iter 8600] [D loss: -365.849853515625] [G loss: 629.9708251953125]\n",
            "[Epoch 8700/10000] [Iter 8700] [D loss: -275.7193603515625] [G loss: -1098.8492431640625]\n",
            "[Epoch 8800/10000] [Iter 8800] [D loss: -513.8250122070312] [G loss: -892.7967529296875]\n",
            "[Epoch 8900/10000] [Iter 8900] [D loss: -48.7205810546875] [G loss: -1025.245849609375]\n",
            "[Epoch 9000/10000] [Iter 9000] [D loss: -421.7381591796875] [G loss: -346.5692138671875]\n",
            "[Epoch 9100/10000] [Iter 9100] [D loss: -228.74639892578125] [G loss: 364.881103515625]\n",
            "[Epoch 9200/10000] [Iter 9200] [D loss: -452.277099609375] [G loss: 61.14390182495117]\n",
            "[Epoch 9300/10000] [Iter 9300] [D loss: -250.76416015625] [G loss: -974.0771484375]\n",
            "[Epoch 9400/10000] [Iter 9400] [D loss: -262.27374267578125] [G loss: -551.4750366210938]\n",
            "[Epoch 9500/10000] [Iter 9500] [D loss: -203.87135314941406] [G loss: 35.88022994995117]\n",
            "[Epoch 9600/10000] [Iter 9600] [D loss: -231.729248046875] [G loss: -550.1500854492188]\n",
            "[Epoch 9700/10000] [Iter 9700] [D loss: -129.5850830078125] [G loss: 29.750234603881836]\n",
            "[Epoch 9800/10000] [Iter 9800] [D loss: -377.8505554199219] [G loss: 74.33919525146484]\n",
            "[Epoch 9900/10000] [Iter 9900] [D loss: -481.123291015625] [G loss: -549.9486083984375]\n"
          ]
        }
      ],
      "source": [
        "from munch import DefaultMunch\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Configurations\n",
        "Config = {\n",
        "    'data_path' : '/content/Dataset',\n",
        "    'year': \"2022\",\n",
        "    'img_size' : 128,\n",
        "    'batch_size_GAN' : 64,\n",
        "    'epochs_GAN': 10000,\n",
        "    'lr_GAN': 0.0001,\n",
        "    'latent_dim' : 32,\n",
        "    'n_critic': 5,\n",
        "    'clip_value':0.01,\n",
        "    'load_G_name' : 'my_G3900.pth',\n",
        "    'epochs_lstm': 30000,\n",
        "    'batch_size_lstm': 4,\n",
        "    'lr_lstm': 0.01,\n",
        "    'J': 8,\n",
        "    'hidden_size' : 64,\n",
        "    'layer': 2,\n",
        "}\n",
        "opt = DefaultMunch.fromDict(Config)\n",
        "\n",
        "# Model Definitions\n",
        "img_shape = (1, opt.img_size, opt.img_size)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(input_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            *block(1024, 2048),\n",
        "            *block(2048, 4096),\n",
        "            nn.Linear(4096, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n",
        "class Dataset_m(Dataset):\n",
        "    def __init__(self, root, transform):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.data_list = glob.glob(f'{self.root}/*jpg')\n",
        "        print(f\"Found {len(self.data_list)} images in {self.root}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data_list[index]\n",
        "        data = Image.open(data).convert('L')\n",
        "        data = self.transform(data)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "# ---------- Main ----------\n",
        "if __name__ == '__main__':\n",
        "    os.makedirs(\"images\", exist_ok=True)\n",
        "    print(opt)\n",
        "\n",
        "    cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "    generator = Generator(input_dim=opt.latent_dim).cuda()\n",
        "    discriminator = Discriminator().cuda()\n",
        "\n",
        "    transforms_train = transforms.Compose([\n",
        "        transforms.Resize((opt.img_size, opt.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=0.5, std=0.5),\n",
        "    ])\n",
        "\n",
        "    my_dataset = Dataset_m(f'{opt.data_path}', transform=transforms_train)\n",
        "    dataloader = DataLoader(dataset=my_dataset, batch_size=opt.batch_size_GAN, shuffle=True)\n",
        "\n",
        "    optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr_GAN)\n",
        "    optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr_GAN)\n",
        "\n",
        "    final_gen_images = []  # To store final 50 images\n",
        "\n",
        "    for epoch in range(opt.epochs_GAN):\n",
        "        for i, imgs in enumerate(dataloader):\n",
        "\n",
        "            real_imgs = imgs.cuda()\n",
        "            iteration = epoch * len(dataloader) + i\n",
        "\n",
        "            # Train Discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            z = torch.randn(imgs.shape[0], opt.latent_dim).cuda()\n",
        "            fake_imgs = generator(z).detach()\n",
        "            fake_of_D = torch.mean(discriminator(fake_imgs))\n",
        "            true_of_D = torch.mean(discriminator(real_imgs))\n",
        "            loss_D = fake_of_D - true_of_D\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            for p in discriminator.parameters():\n",
        "                p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
        "\n",
        "            # Train Generator every n_critic steps\n",
        "            if i % opt.n_critic == 0:\n",
        "                optimizer_G.zero_grad()\n",
        "                gen_imgs = generator(z)\n",
        "                loss_G = -torch.mean(discriminator(gen_imgs))\n",
        "                loss_G.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "                if iteration % 100 == 0:\n",
        "                    print(f\"[Epoch {epoch}/{opt.epochs_GAN}] [Iter {iteration}] [D loss: {loss_D.item()}] [G loss: {loss_G.item()}]\")\n",
        "\n",
        "        # Save model and generated images every 150 epochs\n",
        "        if (epoch + 1) % 150 == 0:\n",
        "            save_model_path = './saved_model'\n",
        "            os.makedirs(save_model_path, exist_ok=True)\n",
        "            torch.save(generator.state_dict(), os.path.join(save_model_path, f'my_G{epoch+1}.pth'))\n",
        "\n",
        "            for j, img in enumerate(gen_imgs[:len(my_dataset)]):  # Save separate images\n",
        "                save_image(img, f\"images/epoch_{epoch+1}_img_{j+1}.png\", normalize=True)\n",
        "\n",
        "        # Save final images (last 50 epochs)\n",
        "        if epoch >= opt.epochs_GAN - 50:\n",
        "            final_gen_images.append(gen_imgs[0].detach().cpu())  # Save only one per epoch\n",
        "\n",
        "    # After training: Save final 50 images\n",
        "    for idx, img in enumerate(final_gen_images):\n",
        "        save_image(img, f\"images/final_{idx+1}.png\", normalize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Step 1: Find only images that start with \"final\"\n",
        "final_images = glob.glob('/content/images/final*.png')\n",
        "print(f\"Total FINAL images: {len(final_images)}\")\n",
        "print(\"First 5 FINAL images:\", final_images[:5])\n",
        "\n",
        "# Step 2: Create a zip file with only final images\n",
        "zip_path = \"/content/final_images.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for file in final_images:\n",
        "        zipf.write(file, arcname=os.path.basename(file))  # Store only the filename, not full path\n",
        "\n",
        "# Step 3: Download the zip file\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "wemFFicTolNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "abab910d-562c-413d-d14a-baf9d8bf14c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FINAL images: 50\n",
            "First 5 FINAL images: ['/content/images/final_3.png', '/content/images/final_46.png', '/content/images/final_26.png', '/content/images/final_33.png', '/content/images/final_22.png']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a66638a-39b6-4e02-92fe-b7fa70c3297a\", \"final_images.zip\", 886351)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}